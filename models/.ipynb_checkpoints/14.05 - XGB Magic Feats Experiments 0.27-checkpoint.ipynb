{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import multiprocessing\n",
    "import difflib\n",
    "import time\n",
    "import gc\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.metrics import log_loss\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from scipy.spatial.distance import cosine, correlation, canberra, chebyshev, minkowski, jaccard, euclidean\n",
    "\n",
    "from xgb_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train():\n",
    "    feats_src = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/data/features/uncleaned_data/'\n",
    "    keras_q1 = np.load(feats_src + 'train_q1_transformed.npy')\n",
    "    keras_q2 = np.load(feats_src + 'train_q2_transformed.npy')\n",
    "    \n",
    "    feats_src2 = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/scripts/features/NER_features/'\n",
    "    keras_q1 = np.load(feats_src2 + 'q1train_NER_128len.npy')\n",
    "    keras_q2 = np.load(feats_src2 + 'q2train_NER_128len.npy')\n",
    "    \n",
    "    xgb_feats = pd.read_csv(feats_src + '/the_1owl/owl_train.csv')\n",
    "    abhishek_feats = pd.read_csv(feats_src + 'abhishek/train_features.csv',\n",
    "                              encoding = 'ISO-8859-1').iloc[:, 2:]\n",
    "    text_feats = pd.read_csv(feats_src + 'other_features/text_features_train.csv',\n",
    "                            encoding = 'ISO-8859-1')\n",
    "    img_feats = pd.read_csv(feats_src + 'other_features/img_features_train.csv')\n",
    "    srk_feats = pd.read_csv(feats_src + 'srk/SRK_grams_features_train.csv')\n",
    "    \n",
    "    mephisto_feats = pd.read_csv('../../data/features/spacylemmat_fullclean/train_mephistopeheles_features.csv').iloc[:, 6:]\n",
    "    #turkewitz_feats = pd.read_csv('../../data/features/lemmat_spacy_features/train_turkewitz_features.csv')\n",
    "    turkewitz_feats = pd.read_csv(feats_src + 'other_features/train_turkewitz_feats_orig.csv')\n",
    "    turkewitz_feats = turkewitz_feats[['q1_freq', 'q2_freq']]\n",
    "    turkewitz_feats['freq_sum'] = turkewitz_feats.q1_freq + turkewitz_feats.q2_freq\n",
    "    turkewitz_feats['freq_diff'] = turkewitz_feats.q1_freq - turkewitz_feats.q2_freq\n",
    "    turkewitz_feats['freq_mult'] = turkewitz_feats.q1_freq * turkewitz_feats.q2_freq\n",
    "    turkewitz_feats['freq_div'] = turkewitz_feats.q1_freq / turkewitz_feats.q2_freq\n",
    "    \n",
    "    xgb_feats.drop(['z_len1', 'z_len2', 'z_word_len1', 'z_word_len2'], axis = 1, inplace = True)\n",
    "    y_train = xgb_feats['is_duplicate']\n",
    "    xgb_feats = xgb_feats.iloc[:, 8:]\n",
    "    \n",
    "    df = pd.concat([xgb_feats, abhishek_feats, text_feats, img_feats, \n",
    "                               turkewitz_feats, mephisto_feats], axis = 1)\n",
    "    df = pd.DataFrame(df)\n",
    "    dfc = df.iloc[0:1000,:]\n",
    "    dfc = dfc.T.drop_duplicates().T\n",
    "    duplicate_cols = sorted(list(set(df.columns).difference(set(dfc.columns))))\n",
    "    print('Dropping duplicate columns:', duplicate_cols)\n",
    "    df.drop(duplicate_cols, axis = 1, inplace = True)\n",
    "    print('Final shape:', df.shape)\n",
    "    \n",
    "    X = np.concatenate([keras_q1, keras_q2, df.values], axis = 1)\n",
    "    X = X.astype('float32')\n",
    "    print('Training data shape:', X.shape)\n",
    "    return X, y_train\n",
    "\n",
    "def labelcount_encode(df2, cols):\n",
    "    df = df2.copy()\n",
    "    categorical_features = cols\n",
    "    new_df = pd.DataFrame()\n",
    "    for cat_feature in categorical_features:\n",
    "        cat_feature_value_counts = df[cat_feature].value_counts()\n",
    "        value_counts_list = cat_feature_value_counts.index.tolist()\n",
    "        value_counts_range_rev = list(reversed(range(len(cat_feature_value_counts)))) # for ascending ordering\n",
    "        value_counts_range = list(range(len(cat_feature_value_counts))) # for descending ordering\n",
    "        labelcount_dict = dict(zip(value_counts_list, value_counts_range))\n",
    "        new_df[cat_feature] = df[cat_feature].map(labelcount_dict)\n",
    "    return new_df\n",
    "\n",
    "def count_encode(df2, cols):\n",
    "    df = df2.copy()\n",
    "    categorical_features = cols\n",
    "    new_df = pd.DataFrame()\n",
    "    for i in categorical_features:\n",
    "        new_df[i] = df[i].astype('object').replace(df[i].value_counts())\n",
    "    return new_df\n",
    "\n",
    "def bin_numerical(df2, cols, step):\n",
    "    df = df2.copy()\n",
    "    numerical_features = cols\n",
    "    new_df = pd.DataFrame()\n",
    "    for i in numerical_features:\n",
    "        feature_range = np.arange(0, np.max(df[i]), step)\n",
    "        new_df[i] = np.digitize(df[i], feature_range, right=True)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_xgb(cv = False):\n",
    "    \n",
    "    t = time.time()\n",
    "    params = {\n",
    "    'seed': 1337,\n",
    "    'colsample_bytree': 0.48,\n",
    "    'silent': 1,\n",
    "    'subsample': 0.74,\n",
    "    'eta': 0.05,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'max_depth': 12,\n",
    "    'min_child_weight': 20,\n",
    "    'nthread': 8,\n",
    "    'tree_method': 'hist',\n",
    "    #'updater': 'grow_gpu',\n",
    "    }\n",
    "    \n",
    "    X_train, y_train = get_train()\n",
    "    \n",
    "    if cv:\n",
    "        dtrain = xgb.DMatrix(X_train, y_train)\n",
    "        hist = xgb.cv(params, dtrain, num_boost_round = 100000, nfold = 5,\n",
    "                      stratified = True, early_stopping_rounds = 350, verbose_eval = 250,\n",
    "                      seed = 1337)\n",
    "        del X_train, y_train\n",
    "        gc.collect()\n",
    "        print('Time it took to train in CV manner:', time.time() - t)\n",
    "        return hist\n",
    "    \n",
    "    else:\n",
    "        X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, stratify = y_train,\n",
    "                                                    test_size = 0.2, random_state = 111)\n",
    "        del X_train, y_train\n",
    "        gc.collect()\n",
    "        dtrain = xgb.DMatrix(X_tr, label = y_tr)\n",
    "        dval = xgb.DMatrix(X_val, label = y_val)\n",
    "        watchlist = [(dtrain, 'train'), (dval, 'valid')]\n",
    "\n",
    "        print('Start training...')\n",
    "        gbm = xgb.train(params, dtrain, 100000, watchlist, \n",
    "                        early_stopping_rounds = 350, verbose_eval = 100)\n",
    "\n",
    "        print('Start predicting...')\n",
    "        val_pred = gbm.predict(xgb.DMatrix(X_val), ntree_limit=gbm.best_ntree_limit)\n",
    "        score = log_loss(y_val, val_pred)\n",
    "        print('Final score:', score, '\\n', 'Time it took to train and predict:', time.time() - t)\n",
    "        \n",
    "        del X_tr, X_val, y_tr, y_val\n",
    "        gc.collect()\n",
    "        return gbm\n",
    "    \n",
    "\n",
    "def run_xgb(model_name, train = True, test = False, cv = False):\n",
    "    if cv:\n",
    "        gbm_hist = train_xgb(True)\n",
    "        return gbm_hist\n",
    "    if train:\n",
    "        gbm = train_xgb()\n",
    "        gbm.save_model('saved_models/XGB/{}.txt'.format(model_name))\n",
    "        if test:\n",
    "            predict_test('{}'.format(model_name))\n",
    "        return gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_transformations_features(transformations_src, mode = 'train'):\n",
    "    print('Adding features based on data transformations.')\n",
    "    lsa10tr_3grams_q1 = np.load(transformations_src + '{}_lsa10_3grams.npy'.format(mode))[0]\n",
    "    lsa10tr_3grams_q2 = np.load(transformations_src + '{}_lsa10_3grams.npy'.format(mode))[1]\n",
    "    \n",
    "    transforms_feats = pd.DataFrame()\n",
    "    transforms_feats['cosine'] = [cosine(x, y) for (x,y) in zip(lsa10tr_3grams_q1, lsa10tr_3grams_q2)]\n",
    "    transforms_feats['correlation'] = [correlation(x, y) for (x,y) in zip(lsa10tr_3grams_q1, lsa10tr_3grams_q2)]\n",
    "    transforms_feats['jaccard'] = [jaccard(x, y) for (x,y) in zip(lsa10tr_3grams_q1, lsa10tr_3grams_q2)]\n",
    "    transforms_feats['euclidean'] = [euclidean(x, y) for (x,y) in zip(lsa10tr_3grams_q1, lsa10tr_3grams_q2)]\n",
    "    transforms_feats['minkowski'] = [minkowski(x, y, 3) for (x,y) in zip(lsa10tr_3grams_q1, lsa10tr_3grams_q2)]\n",
    "    return transforms_feats\n",
    "\n",
    "def get_doc2vec_features(doc2vec_src, mode = 'train'):\n",
    "    print('Adding features based on Doc2Vec distances.')\n",
    "    doc2vec_pre_q1 = np.load(doc2vec_src + '{}_q1_doc2vec_vectors_pretrained.npy'.format(mode))\n",
    "    doc2vec_pre_q2 = np.load(doc2vec_src + '{}_q2_doc2vec_vectors_pretrained.npy'.format(mode))\n",
    "    doc2vec_quora_q1 = np.load(doc2vec_src + '{}_q1_doc2vec_vectors_trainquora.npy'.format(mode))\n",
    "    doc2vec_quora_q2 = np.load(doc2vec_src + '{}_q2_doc2vec_vectors_trainquora.npy'.format(mode))\n",
    "    \n",
    "    d2v_feats_pretrained = pd.DataFrame()\n",
    "    d2v_feats_pretrained['cosine'] = [cosine(x, y) for (x,y) in zip(doc2vec_pre_q1, doc2vec_pre_q2)]\n",
    "    d2v_feats_pretrained['correlation'] = [correlation(x, y) for (x,y) in zip(doc2vec_pre_q1, doc2vec_pre_q2)]\n",
    "    d2v_feats_pretrained['jaccard'] = [jaccard(x, y) for (x,y) in zip(doc2vec_pre_q1, doc2vec_pre_q2)]\n",
    "    d2v_feats_pretrained['euclidean'] = [euclidean(x, y) for (x,y) in zip(doc2vec_pre_q1, doc2vec_pre_q2)]\n",
    "    d2v_feats_pretrained['minkowski'] = [minkowski(x, y, 3) for (x,y) in zip(doc2vec_pre_q1, doc2vec_pre_q2)]\n",
    "    \n",
    "    d2v_feats_quora = pd.DataFrame()\n",
    "    d2v_feats_quora['cosine'] = [cosine(x, y) for (x,y) in zip(doc2vec_quora_q1, doc2vec_quora_q2)]\n",
    "    d2v_feats_quora['correlation'] = [correlation(x, y) for (x,y) in zip(doc2vec_quora_q1, doc2vec_quora_q2)]\n",
    "    d2v_feats_quora['jaccard'] = [jaccard(x, y) for (x,y) in zip(doc2vec_quora_q1, doc2vec_quora_q2)]\n",
    "    d2v_feats_quora['euclidean'] = [euclidean(x, y) for (x,y) in zip(doc2vec_quora_q1, doc2vec_quora_q2)]\n",
    "    d2v_feats_quora['minkowski'] = [minkowski(x, y, 3) for (x,y) in zip(doc2vec_quora_q1, doc2vec_quora_q2)]\n",
    "    return d2v_feats_pretrained, d2v_feats_quora\n",
    "\n",
    "def labelcount_encode(df2, cols):\n",
    "    df = df2.copy()\n",
    "    categorical_features = cols\n",
    "    new_df = pd.DataFrame()\n",
    "    for cat_feature in categorical_features:\n",
    "        cat_feature_value_counts = df[cat_feature].value_counts()\n",
    "        value_counts_list = cat_feature_value_counts.index.tolist()\n",
    "        value_counts_range_rev = list(reversed(range(len(cat_feature_value_counts)))) # for ascending ordering\n",
    "        value_counts_range = list(range(len(cat_feature_value_counts))) # for descending ordering\n",
    "        labelcount_dict = dict(zip(value_counts_list, value_counts_range))\n",
    "        new_df[cat_feature] = df[cat_feature].map(labelcount_dict)\n",
    "    return new_df\n",
    "\n",
    "def count_encode(df2, cols):\n",
    "    df = df2.copy()\n",
    "    categorical_features = cols\n",
    "    new_df = pd.DataFrame()\n",
    "    for i in categorical_features:\n",
    "        new_df[i] = df[i].astype('object').replace(df[i].value_counts())\n",
    "    return new_df\n",
    "\n",
    "def bin_numerical(df2, cols, step):\n",
    "    df = df2.copy()\n",
    "    numerical_features = cols\n",
    "    new_df = pd.DataFrame()\n",
    "    for i in numerical_features:\n",
    "        feature_range = np.arange(0, np.max(df[i]), step)\n",
    "        new_df[i] = np.digitize(df[i], feature_range, right=True)\n",
    "    return new_df\n",
    "\n",
    "def bin_numerical2(df2, cols, step):\n",
    "    df = df2.copy()\n",
    "    numerical_features = cols\n",
    "    for i in numerical_features:\n",
    "        feature_range = np.arange(0, np.max(df[i]), step)\n",
    "        df[i] = pd.cut(df[i], feature_range, right=True)\n",
    "        df[i] = pd.factorize(df[i], sort = True)[0]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "src = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/scripts/features/'\n",
    "train_orig =  pd.read_csv(src + 'df_train_lemmatfullcleanSTEMMED.csv').iloc[:, :-1]\n",
    "test_orig =  pd.read_csv(src + 'df_test_lemmatfullcleanSTEMMED.csv').iloc[:, 4:]\n",
    "full = pd.concat([train_orig, test_orig], ignore_index = True)\n",
    "\n",
    "dflc = labelcount_encode(full, ['question1', 'question2'])\n",
    "lc_cols = ['q1_lc', 'q2_lc']\n",
    "dflc.columns = lc_cols\n",
    "dflc_bin = bin_numerical(dflc, lc_cols, 5000)\n",
    "dflc_bin.columns = ['q1_lc_bin', 'q2_lc_bin']\n",
    "dflc['q1_lc'] = dflc['q1_lc'] / np.max(dflc['q1_lc'])\n",
    "dflc['q2_lc'] = dflc['q2_lc'] / np.max(dflc['q2_lc'])\n",
    "dflc_full = pd.concat([dflc, dflc_bin], axis = 1)\n",
    "\n",
    "dflc_train = dflc_full.iloc[:train_orig.shape[0], :]\n",
    "dflc_test = dflc_full.iloc[train_orig.shape[0]:, :]\n",
    "\n",
    "dflc_test.to_csv('dflc_test.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q1_lc</th>\n",
       "      <th>q2_lc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.697688</td>\n",
       "      <td>0.302575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.003111</td>\n",
       "      <td>0.413824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.483039</td>\n",
       "      <td>0.587203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.129001</td>\n",
       "      <td>0.828601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.021210</td>\n",
       "      <td>0.984841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.527715</td>\n",
       "      <td>0.750727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.517695</td>\n",
       "      <td>0.508366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.382927</td>\n",
       "      <td>0.946819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.168729</td>\n",
       "      <td>0.035021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.385533</td>\n",
       "      <td>0.458736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.345297</td>\n",
       "      <td>0.580698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.153179</td>\n",
       "      <td>0.029992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.680559</td>\n",
       "      <td>0.042616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.041664</td>\n",
       "      <td>0.006541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.004394</td>\n",
       "      <td>0.004366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.003350</td>\n",
       "      <td>0.001587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.204152</td>\n",
       "      <td>0.328404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.863753</td>\n",
       "      <td>0.014047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000282</td>\n",
       "      <td>0.000026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.427495</td>\n",
       "      <td>0.436572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.353702</td>\n",
       "      <td>0.007200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.436812</td>\n",
       "      <td>0.826473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.009603</td>\n",
       "      <td>0.001510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.994082</td>\n",
       "      <td>0.677211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.006438</td>\n",
       "      <td>0.006610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.009640</td>\n",
       "      <td>0.030614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.003907</td>\n",
       "      <td>0.851540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.631534</td>\n",
       "      <td>0.787148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.423510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.001797</td>\n",
       "      <td>0.002077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404260</th>\n",
       "      <td>0.053630</td>\n",
       "      <td>0.000030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404261</th>\n",
       "      <td>0.720322</td>\n",
       "      <td>0.029733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404262</th>\n",
       "      <td>0.615332</td>\n",
       "      <td>0.666039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404263</th>\n",
       "      <td>0.011473</td>\n",
       "      <td>0.213239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404264</th>\n",
       "      <td>0.131125</td>\n",
       "      <td>0.065057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404265</th>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.011561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404266</th>\n",
       "      <td>0.003723</td>\n",
       "      <td>0.005574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404267</th>\n",
       "      <td>0.008478</td>\n",
       "      <td>0.030088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404268</th>\n",
       "      <td>0.049005</td>\n",
       "      <td>0.357692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404269</th>\n",
       "      <td>0.035596</td>\n",
       "      <td>0.025066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404270</th>\n",
       "      <td>0.585236</td>\n",
       "      <td>0.000143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404271</th>\n",
       "      <td>0.946121</td>\n",
       "      <td>0.992758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404272</th>\n",
       "      <td>0.002472</td>\n",
       "      <td>0.003304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404273</th>\n",
       "      <td>0.018919</td>\n",
       "      <td>0.519289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404274</th>\n",
       "      <td>0.009946</td>\n",
       "      <td>0.009598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404275</th>\n",
       "      <td>0.017853</td>\n",
       "      <td>0.188120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404276</th>\n",
       "      <td>0.000917</td>\n",
       "      <td>0.014590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404277</th>\n",
       "      <td>0.187112</td>\n",
       "      <td>0.733879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404278</th>\n",
       "      <td>0.845929</td>\n",
       "      <td>0.004542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404279</th>\n",
       "      <td>0.921596</td>\n",
       "      <td>0.530074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404280</th>\n",
       "      <td>0.288369</td>\n",
       "      <td>0.764559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404281</th>\n",
       "      <td>0.001906</td>\n",
       "      <td>0.001366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404282</th>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404283</th>\n",
       "      <td>0.211586</td>\n",
       "      <td>0.264861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404284</th>\n",
       "      <td>0.558577</td>\n",
       "      <td>0.852418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404285</th>\n",
       "      <td>0.018681</td>\n",
       "      <td>0.015478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404286</th>\n",
       "      <td>0.000752</td>\n",
       "      <td>0.045712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404287</th>\n",
       "      <td>0.624672</td>\n",
       "      <td>0.701571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404288</th>\n",
       "      <td>0.128485</td>\n",
       "      <td>0.234096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404289</th>\n",
       "      <td>0.750408</td>\n",
       "      <td>0.171545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>404290 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           q1_lc     q2_lc\n",
       "0       0.697688  0.302575\n",
       "1       0.003111  0.413824\n",
       "2       0.483039  0.587203\n",
       "3       0.129001  0.828601\n",
       "4       0.021210  0.984841\n",
       "5       0.527715  0.750727\n",
       "6       0.517695  0.508366\n",
       "7       0.382927  0.946819\n",
       "8       0.168729  0.035021\n",
       "9       0.385533  0.458736\n",
       "10      0.345297  0.580698\n",
       "11      0.153179  0.029992\n",
       "12      0.680559  0.042616\n",
       "13      0.041664  0.006541\n",
       "14      0.004394  0.004366\n",
       "15      0.003350  0.001587\n",
       "16      0.204152  0.328404\n",
       "17      0.863753  0.014047\n",
       "18      0.000282  0.000026\n",
       "19      0.427495  0.436572\n",
       "20      0.353702  0.007200\n",
       "21      0.436812  0.826473\n",
       "22      0.009603  0.001510\n",
       "23      0.994082  0.677211\n",
       "24      0.006438  0.006610\n",
       "25      0.009640  0.030614\n",
       "26      0.003907  0.851540\n",
       "27      0.631534  0.787148\n",
       "28      0.000168  0.423510\n",
       "29      0.001797  0.002077\n",
       "...          ...       ...\n",
       "404260  0.053630  0.000030\n",
       "404261  0.720322  0.029733\n",
       "404262  0.615332  0.666039\n",
       "404263  0.011473  0.213239\n",
       "404264  0.131125  0.065057\n",
       "404265  0.000228  0.011561\n",
       "404266  0.003723  0.005574\n",
       "404267  0.008478  0.030088\n",
       "404268  0.049005  0.357692\n",
       "404269  0.035596  0.025066\n",
       "404270  0.585236  0.000143\n",
       "404271  0.946121  0.992758\n",
       "404272  0.002472  0.003304\n",
       "404273  0.018919  0.519289\n",
       "404274  0.009946  0.009598\n",
       "404275  0.017853  0.188120\n",
       "404276  0.000917  0.014590\n",
       "404277  0.187112  0.733879\n",
       "404278  0.845929  0.004542\n",
       "404279  0.921596  0.530074\n",
       "404280  0.288369  0.764559\n",
       "404281  0.001906  0.001366\n",
       "404282  0.000223  0.000948\n",
       "404283  0.211586  0.264861\n",
       "404284  0.558577  0.852418\n",
       "404285  0.018681  0.015478\n",
       "404286  0.000752  0.045712\n",
       "404287  0.624672  0.701571\n",
       "404288  0.128485  0.234096\n",
       "404289  0.750408  0.171545\n",
       "\n",
       "[404290 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dflc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Column not found: q1_lc_bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-de571607331a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtf_dflc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdflc_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mturkewitz_feats\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtf_dflc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'q1gr1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_dflc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'q1_freq'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'q1_lc_bin'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtf_dflc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'q2gr2'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_dflc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'q2_freq'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'q2_lc_bin'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtf_dflc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'q1gr1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_dflc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'q1gr1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_dflc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'q1gr1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/w/anaconda3/envs/idp3/lib/python3.5/site-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Column not found: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gotitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Column not found: q1_lc_bin'"
     ]
    }
   ],
   "source": [
    "#turkewitz_feats = pd.read_csv('/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/data/features/uncleaned_data/other_features/train_turkewitz_feats_orig.csv')\n",
    "turkewitz_feats = pd.read_csv('/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/data/features/spacylemmat_fullclean/train_turkewitz_features.csv')\n",
    "turkewitz_feats = turkewitz_feats[['q1_freq', 'q2_freq']]\n",
    "\n",
    "tf_dflc = pd.concat([dflc_train, turkewitz_feats], axis = 1)\n",
    "tf_dflc['q1gr1'] = tf_dflc.groupby(['q1_freq'])['q1_lc_bin'].transform('mean')\n",
    "tf_dflc['q2gr2'] = tf_dflc.groupby(['q2_freq'])['q2_lc_bin'].transform('mean')\n",
    "tf_dflc['q1gr1'] = tf_dflc['q1gr1'] / np.max(tf_dflc['q1gr1'])\n",
    "tf_dflc['q2gr2'] = tf_dflc['q2gr2'] / np.max(tf_dflc['q2gr2'])\n",
    "\n",
    "ff1 = turkewitz_feats.groupby(['q2_freq'])['q1_freq'].transform('sum')\n",
    "ff2 = turkewitz_feats.groupby(['q1_freq'])['q2_freq'].transform('sum')\n",
    "ff1 = ff1 / np.max(ff1)\n",
    "ff2 = ff2 / np.max(ff2)\n",
    "ff1m = turkewitz_feats.groupby(['q2_freq'])['q1_freq'].transform('mean')\n",
    "ff2m = turkewitz_feats.groupby(['q1_freq'])['q2_freq'].transform('mean')\n",
    "ff1m = ff1m / np.max(ff1m)\n",
    "ff2m = ff2m / np.max(ff2m)\n",
    "gr_feats = pd.DataFrame()\n",
    "gr_feats['ff1'] = ff1\n",
    "gr_feats['ff2'] = ff2\n",
    "gr_feats['ff1m'] = ff1m\n",
    "gr_feats['ff2m'] = ff2m\n",
    "\n",
    "train_lc3 = labelcount_encode(turkewitz_feats, ['q1_freq', 'q2_freq'])\n",
    "train_c = count_encode(turkewitz_feats, ['q1_freq', 'q2_freq'])\n",
    "train_c.q1_freq = train_c.q1_freq / np.max(train_c.q1_freq)\n",
    "train_c.q2_freq = train_c.q2_freq / np.max(train_c.q2_freq)\n",
    "\n",
    "\n",
    "new_feats = np.concatenate([train_c, train_lc3, gr_feats, tf_dflc, dflc_train], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = get_train()\n",
    "d2v_pre = np.load('train_doc2vec_pretrained_distances.npy')\n",
    "d2v_quora = np.load('train_doc2vec_quoratrain_distances.npy')\n",
    "transforms = np.load('train_transformations_distances.npy')\n",
    "\n",
    "X_train = np.concatenate([X_train, d2v_pre, d2v_quora, transforms, new_feats], axis = 1)\n",
    "X_train = X_train.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'seed': 1337,\n",
    "    'colsample_bytree': 0.48,\n",
    "    'silent': 1,\n",
    "    'subsample': 0.74,\n",
    "    'eta': 0.05,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'max_depth': 12,\n",
    "    'min_child_weight': 20,\n",
    "    'nthread': 8,\n",
    "    'tree_method': 'hist',\n",
    "    }\n",
    "\n",
    "t = time.time()\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, stratify = y_train,\n",
    "                                            test_size = 0.2, random_state = 111)\n",
    "dtrain = xgb.DMatrix(X_tr, label = y_tr)\n",
    "dval = xgb.DMatrix(X_val, label = y_val)\n",
    "watchlist = [(dtrain, 'train'), (dval, 'valid')]\n",
    "\n",
    "print('Start training...')\n",
    "gbm = xgb.train(params, dtrain, 100000, watchlist, \n",
    "                early_stopping_rounds = 150, verbose_eval = 100)\n",
    "\n",
    "print('Start predicting...')\n",
    "val_pred = gbm.predict(xgb.DMatrix(X_val), ntree_limit=gbm.best_ntree_limit)\n",
    "score = log_loss(y_val, val_pred)\n",
    "print('Final score:', score, '\\n', 'Time it took to train and predict:', time.time() - t)\n",
    "gbm.save_model('saved_models/XGB/XGB_turkewitz_Doc2Vec2_LSA_GroupedFeats_experiments_sortedBIN.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
